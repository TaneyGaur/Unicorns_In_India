{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90477467",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:25.989549Z",
     "iopub.status.busy": "2024-11-28T07:19:25.989082Z",
     "iopub.status.idle": "2024-11-28T07:19:27.007455Z",
     "shell.execute_reply": "2024-11-28T07:19:27.006022Z"
    },
    "papermill": {
     "duration": 1.029329,
     "end_time": "2024-11-28T07:19:27.010072",
     "exception": false,
     "start_time": "2024-11-28T07:19:25.980743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/store-sales-time-series-forecasting/oil.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/sample_submission.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/stores.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/train.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/test.csv\n",
      "/kaggle/input/store-sales-time-series-forecasting/transactions.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f63daa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:27.021625Z",
     "iopub.status.busy": "2024-11-28T07:19:27.021066Z",
     "iopub.status.idle": "2024-11-28T07:19:32.355651Z",
     "shell.execute_reply": "2024-11-28T07:19:32.354107Z"
    },
    "papermill": {
     "duration": 5.343561,
     "end_time": "2024-11-28T07:19:32.358619",
     "exception": false,
     "start_time": "2024-11-28T07:19:27.015058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/store-sales-time-series-forecasting/test.csv\")\n",
    "transactions = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/transactions.csv')\n",
    "stores = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/stores.csv')\n",
    "oil = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/oil.csv')\n",
    "holidays_events = pd.read_csv('/kaggle/input/store-sales-time-series-forecasting/holidays_events.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e6088",
   "metadata": {
    "papermill": {
     "duration": 0.007817,
     "end_time": "2024-11-28T07:19:32.376473",
     "exception": false,
     "start_time": "2024-11-28T07:19:32.368656",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After importing all the required datasets it is time to focus on the preprocessing of the datasets.\n",
    "Preprocessing of Datasets involves 2 major things:\n",
    "1. Looking if it contains any null values\n",
    "2. Conversion of Datatypes into forms that will be acceptible to the model for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20f13c87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:32.396238Z",
     "iopub.status.busy": "2024-11-28T07:19:32.395291Z",
     "iopub.status.idle": "2024-11-28T07:19:32.742948Z",
     "shell.execute_reply": "2024-11-28T07:19:32.741489Z"
    },
    "papermill": {
     "duration": 0.361214,
     "end_time": "2024-11-28T07:19:32.745407",
     "exception": false,
     "start_time": "2024-11-28T07:19:32.384193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "date           0\n",
       "store_nbr      0\n",
       "family         0\n",
       "onpromotion    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()\n",
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43731e",
   "metadata": {
    "papermill": {
     "duration": 0.004571,
     "end_time": "2024-11-28T07:19:32.755343",
     "exception": false,
     "start_time": "2024-11-28T07:19:32.750772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As of now there are no null values in the datasets so we are going to merge the datasets but we will have to convert categorical features into a form that the model accepts and convert data regarding dates to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ebeaa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:32.767438Z",
     "iopub.status.busy": "2024-11-28T07:19:32.767012Z",
     "iopub.status.idle": "2024-11-28T07:19:37.309042Z",
     "shell.execute_reply": "2024-11-28T07:19:37.302609Z"
    },
    "papermill": {
     "duration": 4.552707,
     "end_time": "2024-11-28T07:19:37.312797",
     "exception": false,
     "start_time": "2024-11-28T07:19:32.760090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.merge(train, stores, how='left', on='store_nbr')\n",
    "train = pd.merge(train, transactions, how='left', on=['date', 'store_nbr'])\n",
    "train = pd.merge(train, oil, how='left', on='date')\n",
    "train = pd.merge(train, holidays_events, how='left', on='date')\n",
    "\n",
    "test = pd.merge(test, stores, how='left', on='store_nbr')\n",
    "test = pd.merge(test, transactions, how='left', on=['date', 'store_nbr'])\n",
    "test = pd.merge(test, oil, how='left', on='date')\n",
    "test = pd.merge(test, holidays_events, how='left', on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f81f2a",
   "metadata": {
    "papermill": {
     "duration": 0.007908,
     "end_time": "2024-11-28T07:19:37.329213",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.321305",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we have merged the training and testing datasets with features of interests.\n",
    "Now is time to feature engineer and convert date into day of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e78adb9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:37.343314Z",
     "iopub.status.busy": "2024-11-28T07:19:37.342830Z",
     "iopub.status.idle": "2024-11-28T07:19:37.788036Z",
     "shell.execute_reply": "2024-11-28T07:19:37.786757Z"
    },
    "papermill": {
     "duration": 0.45463,
     "end_time": "2024-11-28T07:19:37.791104",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.336474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['day_of_week'] = pd.to_datetime(train['date']).dt.dayofweek\n",
    "test['day_of_week'] = pd.to_datetime(test['date']).dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f88aa",
   "metadata": {
    "papermill": {
     "duration": 0.004717,
     "end_time": "2024-11-28T07:19:37.804770",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.800053",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's check if our data is skewed in any manner...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25847a95",
   "metadata": {
    "papermill": {
     "duration": 0.00506,
     "end_time": "2024-11-28T07:19:37.815083",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.810023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we see that a tail is getting formed in the datset....meaning that the data is skewed or concenterated on one timeframe...we are going to fix this by using a log(1+x) function...it provides weight to smaller values while at the dame time lowering the larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316cbb98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:37.827141Z",
     "iopub.status.busy": "2024-11-28T07:19:37.826621Z",
     "iopub.status.idle": "2024-11-28T07:19:37.894808Z",
     "shell.execute_reply": "2024-11-28T07:19:37.893608Z"
    },
    "papermill": {
     "duration": 0.077401,
     "end_time": "2024-11-28T07:19:37.897348",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.819947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['log_sales'] = np.log1p(train['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660e5990",
   "metadata": {
    "papermill": {
     "duration": 0.004487,
     "end_time": "2024-11-28T07:19:37.906665",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.902178",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that we have dealt with the skewness of the dataset we are going to do feature selection for training our model.\n",
    "But as some of the features may have categorical values that the model won't accept we will use get dummies functions for converting these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e97618d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:37.917821Z",
     "iopub.status.busy": "2024-11-28T07:19:37.917429Z",
     "iopub.status.idle": "2024-11-28T07:19:38.545225Z",
     "shell.execute_reply": "2024-11-28T07:19:38.544122Z"
    },
    "papermill": {
     "duration": 0.636712,
     "end_time": "2024-11-28T07:19:38.547967",
     "exception": false,
     "start_time": "2024-11-28T07:19:37.911255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = ['store_nbr', 'onpromotion', 'day_of_week', 'cluster', 'dcoilwtico']\n",
    "\n",
    "X_train = pd.get_dummies(train[features])\n",
    "y_train = train['log_sales']\n",
    "\n",
    "X_train = X_train.dropna()\n",
    "y_train = y_train[X_train.index]  \n",
    "X_test = pd.get_dummies(test[features])\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181221e9",
   "metadata": {
    "papermill": {
     "duration": 0.004475,
     "end_time": "2024-11-28T07:19:38.557346",
     "exception": false,
     "start_time": "2024-11-28T07:19:38.552871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now the model we are going to use is GradientBoostingRegressor model... And calculate the rmsle value as it is the criteria for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78243081",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:19:38.568846Z",
     "iopub.status.busy": "2024-11-28T07:19:38.568370Z",
     "iopub.status.idle": "2024-11-28T07:22:25.972181Z",
     "shell.execute_reply": "2024-11-28T07:22:25.970704Z"
    },
    "papermill": {
     "duration": 167.412809,
     "end_time": "2024-11-28T07:22:25.975238",
     "exception": false,
     "start_time": "2024-11-28T07:19:38.562429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_predict = model.predict(X_valid)\n",
    "\n",
    "rmsle = np.sqrt(mean_squared_error(y_valid, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376e82d2",
   "metadata": {
    "papermill": {
     "duration": 0.004412,
     "end_time": "2024-11-28T07:22:25.985202",
     "exception": false,
     "start_time": "2024-11-28T07:22:25.980790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now a new problem that arises with encoding categorical data is that both the training and test dataset should have identical features and data count otherwise the encoding will be incorrect and leading to model rejecting the input.\n",
    "So to counter this problem we will recreate each column missing in test dataset with all it's value set to zero..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "417ac2e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:22:25.996933Z",
     "iopub.status.busy": "2024-11-28T07:22:25.996541Z",
     "iopub.status.idle": "2024-11-28T07:22:26.005238Z",
     "shell.execute_reply": "2024-11-28T07:22:26.003842Z"
    },
    "papermill": {
     "duration": 0.017544,
     "end_time": "2024-11-28T07:22:26.007903",
     "exception": false,
     "start_time": "2024-11-28T07:22:25.990359",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_train = ['onpromotion', 'cluster', 'dcoilwtico', 'store_nbr', 'day_of_week']\n",
    "\n",
    "X_Test_encoded = pd.get_dummies(X_test)\n",
    "\n",
    "missing_cols = set(X_train.columns) - set(X_Test_encoded.columns)\n",
    "for col in missing_cols:\n",
    "    X_Test_encoded[col] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7936453f",
   "metadata": {
    "papermill": {
     "duration": 0.004464,
     "end_time": "2024-11-28T07:22:26.017199",
     "exception": false,
     "start_time": "2024-11-28T07:22:26.012735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that there will be null values in test dataset we will use an imputer to fill those null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c86850",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:22:26.028430Z",
     "iopub.status.busy": "2024-11-28T07:22:26.028033Z",
     "iopub.status.idle": "2024-11-28T07:22:26.044166Z",
     "shell.execute_reply": "2024-11-28T07:22:26.042984Z"
    },
    "papermill": {
     "duration": 0.024297,
     "end_time": "2024-11-28T07:22:26.046432",
     "exception": false,
     "start_time": "2024-11-28T07:22:26.022135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_nbr      0\n",
      "onpromotion    0\n",
      "day_of_week    0\n",
      "cluster        0\n",
      "dcoilwtico     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "X_Test_encoded = pd.DataFrame(imputer.fit_transform(X_Test_encoded), columns=X_Test_encoded.columns)\n",
    "print(X_Test_encoded.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47e3e117",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:22:26.057684Z",
     "iopub.status.busy": "2024-11-28T07:22:26.057268Z",
     "iopub.status.idle": "2024-11-28T07:22:26.091749Z",
     "shell.execute_reply": "2024-11-28T07:22:26.090563Z"
    },
    "papermill": {
     "duration": 0.043041,
     "end_time": "2024-11-28T07:22:26.094254",
     "exception": false,
     "start_time": "2024-11-28T07:22:26.051213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_test = ['store_nbr', 'onpromotion', 'day_of_week', 'cluster', 'dcoilwtico']\n",
    "X_Test = X_test[features]\n",
    "\n",
    "# Predictions on the test set\n",
    "y_predict_test = model.predict(X_Test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2bdb440",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:22:26.105743Z",
     "iopub.status.busy": "2024-11-28T07:22:26.105303Z",
     "iopub.status.idle": "2024-11-28T07:22:26.177978Z",
     "shell.execute_reply": "2024-11-28T07:22:26.176604Z"
    },
    "papermill": {
     "duration": 0.081267,
     "end_time": "2024-11-28T07:22:26.180612",
     "exception": false,
     "start_time": "2024-11-28T07:22:26.099345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'sales': y_predict_test\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0047e3b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-28T07:22:26.191940Z",
     "iopub.status.busy": "2024-11-28T07:22:26.191566Z",
     "iopub.status.idle": "2024-11-28T07:22:26.206819Z",
     "shell.execute_reply": "2024-11-28T07:22:26.205502Z"
    },
    "papermill": {
     "duration": 0.023837,
     "end_time": "2024-11-28T07:22:26.209437",
     "exception": false,
     "start_time": "2024-11-28T07:22:26.185600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000888</td>\n",
       "      <td>2.396544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000889</td>\n",
       "      <td>2.396544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000890</td>\n",
       "      <td>4.970418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000891</td>\n",
       "      <td>6.784168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000892</td>\n",
       "      <td>2.396544</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id     sales\n",
       "0  3000888  2.396544\n",
       "1  3000889  2.396544\n",
       "2  3000890  4.970418\n",
       "3  3000891  6.784168\n",
       "4  3000892  2.396544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c0a9c5",
   "metadata": {
    "papermill": {
     "duration": 0.004731,
     "end_time": "2024-11-28T07:22:26.219313",
     "exception": false,
     "start_time": "2024-11-28T07:22:26.214582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Reference: https://www.kaggle.com/code/bravo03/store-sales-time-series-forecasting?kernelSessionId=160694314"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 2887556,
     "sourceId": 29781,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 183.901777,
   "end_time": "2024-11-28T07:22:26.947258",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-28T07:19:23.045481",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
